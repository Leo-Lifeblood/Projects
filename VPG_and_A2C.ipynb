{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHBtbBDZFtJKxwap4GIt6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leo-Lifeblood/Projects/blob/main/VPG_and_A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WML9tF2SIAX",
        "outputId": "a664b2c2-a984-4099-f004-64c6583f0305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_\n",
        "import numpy as np\n",
        "\n",
        "#this worked out of the box the changes I made are just technical\n",
        "\n",
        "GAMMA = 0.99\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "\n",
        "class XavierLinear(nn.Linear):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        xavier_uniform_(self.weight)\n",
        "\n",
        "\n",
        "class VPG(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(VPG, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            XavierLinear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            XavierLinear(128, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softmax(self.net(x), dim=0)\n",
        "\n",
        "\n",
        "def run_episode(model, env):\n",
        "    obs = env.reset()[0]\n",
        "    obs = torch.Tensor(env.reset()[0]).to(DEVICE)\n",
        "    te = tr = False\n",
        "    rewards, outputs, actions = [], [], []\n",
        "    while not (te or tr):\n",
        "        probs = model(obs)\n",
        "        action = probs.multinomial(1).item()\n",
        "        obs, r, te, tr, _ = env.step(action)\n",
        "        obs = torch.Tensor(obs).to(DEVICE)\n",
        "        if (te or tr):\n",
        "            r = 0 # Here I also tried -1 if the pole falls and 0 otherwise\n",
        "        rewards.append(r)\n",
        "        outputs.append(probs)\n",
        "        actions.append(action)\n",
        "    return torch.Tensor(rewards).to(DEVICE), torch.concatenate(outputs).reshape(len(rewards), 2), actions\n",
        "\n",
        "def discount_rewards(rewards):\n",
        "    discounted_r = torch.zeros_like(rewards)\n",
        "    additive_r = 0\n",
        "    for idx in range(len(rewards)-1, -1, -1):\n",
        "        to_add = GAMMA * additive_r\n",
        "        additive_r = to_add + rewards[idx]\n",
        "        discounted_r[idx] = additive_r\n",
        "    return discounted_r.to(DEVICE)\n",
        "\n",
        "def loss_function(discounted_r, probs, actions):\n",
        "    logprobs = torch.log(probs)\n",
        "    selected = logprobs[range(probs.shape[0]), actions]\n",
        "    discounted_r = (discounted_r - discounted_r.mean()) / discounted_r.std() #take this out of the comment\n",
        "    weighted = selected * discounted_r\n",
        "    return -weighted.mean() # change this to mean from sum so that larger batch sizes dont do damage it actually makes convergence slightly slower but\n",
        "\n",
        "# The actual training loop:\n",
        "\n",
        "episode_total_reward = 0\n",
        "batch_losses = torch.Tensor().to(DEVICE)\n",
        "batch_actions = []\n",
        "batch_disc_r = torch.Tensor().to(DEVICE)\n",
        "batch_probs = torch.Tensor().to(DEVICE)\n",
        "best_ep_reward = 0\n",
        "losses, ep_total_lenghts = [], [0]\n",
        "\n",
        "episodes = 0\n",
        "TARGET_REWARD = 100\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "model = VPG(env.observation_space.shape[0],\n",
        "            2).to(DEVICE)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "while np.array(ep_total_lenghts)[-100:].mean() < TARGET_REWARD:\n",
        "    rewards, probs, actions = run_episode(model, env)\n",
        "    discounted_r = discount_rewards(rewards)\n",
        "    episode_total_reward = rewards.shape[0]\n",
        "    ep_total_lenghts.append(episode_total_reward)\n",
        "    episodes += 1\n",
        "    batch_actions += actions\n",
        "    batch_disc_r = torch.concatenate([batch_disc_r, discounted_r])\n",
        "    batch_probs = torch.concatenate([batch_probs, probs])\n",
        "\n",
        "    if episodes % BATCH_SIZE == 0:\n",
        "        loss = loss_function(batch_disc_r, batch_probs, batch_actions)\n",
        "        losses.append(loss.item())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.001) # this doesnt need to be here but I added it for the idea of stability\n",
        "        optim.step()\n",
        "        batch_actions = []\n",
        "        batch_disc_r = torch.Tensor().to(DEVICE)\n",
        "        batch_probs = torch.Tensor().to(DEVICE)\n",
        "        print(f\"Episode {episodes}. Loss: {loss}. Reward: {episode_total_reward}\")\n",
        "print(f\"Success in {episodes} episodes. Loss: {loss}. Reward: {episode_total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyqmKyQteIZ-",
        "outputId": "c0fe607b-4959-4ae6-c261-c4748292bf67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 4. Loss: -0.008065112866461277. Reward: 22\n",
            "Episode 8. Loss: 0.015668725594878197. Reward: 24\n",
            "Episode 12. Loss: 0.048676639795303345. Reward: 14\n",
            "Episode 16. Loss: -0.03799426183104515. Reward: 13\n",
            "Episode 20. Loss: -0.061613745987415314. Reward: 11\n",
            "Episode 24. Loss: 0.005115847568958998. Reward: 23\n",
            "Episode 28. Loss: 0.009838522411882877. Reward: 38\n",
            "Episode 32. Loss: 0.005891811568289995. Reward: 13\n",
            "Episode 36. Loss: 0.007423750590533018. Reward: 14\n",
            "Episode 40. Loss: -0.001050323247909546. Reward: 28\n",
            "Episode 44. Loss: 0.00690933782607317. Reward: 28\n",
            "Episode 48. Loss: 0.007801996544003487. Reward: 15\n",
            "Episode 52. Loss: 0.008426588959991932. Reward: 21\n",
            "Episode 56. Loss: -0.014807883650064468. Reward: 37\n",
            "Episode 60. Loss: 0.010726138018071651. Reward: 15\n",
            "Episode 64. Loss: -0.04424520209431648. Reward: 17\n",
            "Episode 68. Loss: 0.0066779600456357. Reward: 25\n",
            "Episode 72. Loss: -0.004824013914912939. Reward: 26\n",
            "Episode 76. Loss: -6.550866964971647e-05. Reward: 30\n",
            "Episode 80. Loss: -0.01940113492310047. Reward: 54\n",
            "Episode 84. Loss: 0.001504808315075934. Reward: 56\n",
            "Episode 88. Loss: -0.004304284229874611. Reward: 15\n",
            "Episode 92. Loss: -0.008557087741792202. Reward: 12\n",
            "Episode 96. Loss: -0.03247632831335068. Reward: 63\n",
            "Episode 100. Loss: -0.006262036971747875. Reward: 84\n",
            "Episode 104. Loss: -0.0025826108176261187. Reward: 15\n",
            "Episode 108. Loss: -0.008523843251168728. Reward: 27\n",
            "Episode 112. Loss: 0.003253370290622115. Reward: 30\n",
            "Episode 116. Loss: -0.00458265095949173. Reward: 91\n",
            "Episode 120. Loss: 0.0008115512900985777. Reward: 22\n",
            "Episode 124. Loss: -0.0114149060100317. Reward: 31\n",
            "Episode 128. Loss: -0.021920422092080116. Reward: 36\n",
            "Episode 132. Loss: -0.01338101364672184. Reward: 30\n",
            "Episode 136. Loss: -0.03049749694764614. Reward: 21\n",
            "Episode 140. Loss: -0.005452572368085384. Reward: 39\n",
            "Episode 144. Loss: -0.024969905614852905. Reward: 33\n",
            "Episode 148. Loss: -0.004976643715053797. Reward: 22\n",
            "Episode 152. Loss: -0.008993101306259632. Reward: 26\n",
            "Episode 156. Loss: -0.016779137775301933. Reward: 105\n",
            "Episode 160. Loss: -0.004840602166950703. Reward: 27\n",
            "Episode 164. Loss: -0.031661808490753174. Reward: 23\n",
            "Episode 168. Loss: 0.005212720949202776. Reward: 67\n",
            "Episode 172. Loss: 0.0036640611942857504. Reward: 40\n",
            "Episode 176. Loss: -0.003823615610599518. Reward: 36\n",
            "Episode 180. Loss: -0.032724786549806595. Reward: 32\n",
            "Episode 184. Loss: -0.005728486459702253. Reward: 29\n",
            "Episode 188. Loss: -0.013836669735610485. Reward: 14\n",
            "Episode 192. Loss: -0.004155953414738178. Reward: 56\n",
            "Episode 196. Loss: 0.0002086957247229293. Reward: 15\n",
            "Episode 200. Loss: -0.03564782440662384. Reward: 14\n",
            "Episode 204. Loss: 0.01898782327771187. Reward: 28\n",
            "Episode 208. Loss: -0.027764983475208282. Reward: 21\n",
            "Episode 212. Loss: -0.007092456799000502. Reward: 116\n",
            "Episode 216. Loss: 0.011171248741447926. Reward: 28\n",
            "Episode 220. Loss: -0.010912244208157063. Reward: 11\n",
            "Episode 224. Loss: 0.009399156086146832. Reward: 49\n",
            "Episode 228. Loss: -0.01780843921005726. Reward: 45\n",
            "Episode 232. Loss: -0.05060182884335518. Reward: 32\n",
            "Episode 236. Loss: -0.01910572499036789. Reward: 92\n",
            "Episode 240. Loss: -0.022045107558369637. Reward: 104\n",
            "Episode 244. Loss: -0.004503089934587479. Reward: 23\n",
            "Episode 248. Loss: -0.018969934433698654. Reward: 46\n",
            "Episode 252. Loss: -0.021479545161128044. Reward: 24\n",
            "Episode 256. Loss: -0.024066224694252014. Reward: 19\n",
            "Episode 260. Loss: -0.05522649362683296. Reward: 84\n",
            "Episode 264. Loss: -0.028683479875326157. Reward: 33\n",
            "Episode 268. Loss: -0.01786513440310955. Reward: 22\n",
            "Episode 272. Loss: -0.001441213651560247. Reward: 55\n",
            "Episode 276. Loss: -0.031976450234651566. Reward: 30\n",
            "Episode 280. Loss: -0.020740998908877373. Reward: 29\n",
            "Episode 284. Loss: 0.0017501538386568427. Reward: 66\n",
            "Episode 288. Loss: -0.03125132992863655. Reward: 33\n",
            "Episode 292. Loss: -0.05126853287220001. Reward: 43\n",
            "Episode 296. Loss: -0.004030960146337748. Reward: 36\n",
            "Episode 300. Loss: 0.002582231070846319. Reward: 35\n",
            "Episode 304. Loss: -0.009150960482656956. Reward: 164\n",
            "Episode 308. Loss: 0.003422396257519722. Reward: 55\n",
            "Episode 312. Loss: -0.006696552503854036. Reward: 77\n",
            "Episode 316. Loss: 0.0024897283874452114. Reward: 69\n",
            "Episode 320. Loss: -0.004057725891470909. Reward: 113\n",
            "Episode 324. Loss: -0.011352090165019035. Reward: 46\n",
            "Episode 328. Loss: -0.001558930380269885. Reward: 46\n",
            "Episode 332. Loss: -0.0067075141705572605. Reward: 35\n",
            "Episode 336. Loss: -0.05282019451260567. Reward: 68\n",
            "Episode 340. Loss: -0.010040152817964554. Reward: 93\n",
            "Episode 344. Loss: -0.022378170862793922. Reward: 67\n",
            "Episode 348. Loss: -0.04477761313319206. Reward: 122\n",
            "Episode 352. Loss: -0.024696819484233856. Reward: 91\n",
            "Episode 356. Loss: -0.01645560748875141. Reward: 80\n",
            "Episode 360. Loss: -0.01542985625565052. Reward: 55\n",
            "Episode 364. Loss: -0.014174256473779678. Reward: 87\n",
            "Episode 368. Loss: 0.00968586839735508. Reward: 115\n",
            "Episode 372. Loss: -0.02822362631559372. Reward: 40\n",
            "Episode 376. Loss: -0.0027523476164788008. Reward: 70\n",
            "Episode 380. Loss: -0.018369605764746666. Reward: 99\n",
            "Episode 384. Loss: -0.0038542412221431732. Reward: 78\n",
            "Episode 388. Loss: -0.023472081869840622. Reward: 67\n",
            "Episode 392. Loss: -0.010655643418431282. Reward: 45\n",
            "Episode 396. Loss: -0.04075738415122032. Reward: 69\n",
            "Episode 400. Loss: -0.05128847435116768. Reward: 28\n",
            "Episode 404. Loss: -0.006386770401149988. Reward: 140\n",
            "Episode 408. Loss: 0.0005581898731179535. Reward: 34\n",
            "Episode 412. Loss: -0.0034368212800472975. Reward: 87\n",
            "Episode 416. Loss: -0.009053286164999008. Reward: 138\n",
            "Episode 420. Loss: -0.012251833453774452. Reward: 165\n",
            "Episode 424. Loss: -0.004277796018868685. Reward: 116\n",
            "Episode 428. Loss: -0.01434006355702877. Reward: 95\n",
            "Episode 432. Loss: -0.0228881873190403. Reward: 29\n",
            "Episode 436. Loss: -0.01674472726881504. Reward: 112\n",
            "Episode 440. Loss: -0.03051561489701271. Reward: 204\n",
            "Episode 444. Loss: -0.0037714720238000154. Reward: 63\n",
            "Episode 448. Loss: -0.03133855760097504. Reward: 185\n",
            "Episode 452. Loss: -0.019689572975039482. Reward: 19\n",
            "Episode 456. Loss: -0.010381409898400307. Reward: 204\n",
            "Episode 460. Loss: -0.013815775513648987. Reward: 28\n",
            "Episode 464. Loss: -0.015061791054904461. Reward: 31\n",
            "Episode 468. Loss: -0.02256711758673191. Reward: 53\n",
            "Episode 472. Loss: -0.012244017794728279. Reward: 133\n",
            "Episode 476. Loss: -0.013142741285264492. Reward: 54\n",
            "Episode 480. Loss: -0.011359822005033493. Reward: 30\n",
            "Episode 484. Loss: -0.026487981900572777. Reward: 40\n",
            "Episode 488. Loss: -0.009033380076289177. Reward: 52\n",
            "Episode 492. Loss: -0.017871519550681114. Reward: 114\n",
            "Episode 496. Loss: -0.036882609128952026. Reward: 125\n",
            "Episode 500. Loss: -0.02436457760632038. Reward: 66\n",
            "Episode 504. Loss: -0.0069459229707717896. Reward: 103\n",
            "Episode 508. Loss: -0.010296002961695194. Reward: 101\n",
            "Episode 512. Loss: -0.026462741196155548. Reward: 66\n",
            "Episode 516. Loss: -0.011764926835894585. Reward: 175\n",
            "Episode 520. Loss: -0.04750523716211319. Reward: 92\n",
            "Episode 524. Loss: -0.02822323888540268. Reward: 63\n",
            "Episode 528. Loss: -0.013950660824775696. Reward: 177\n",
            "Episode 532. Loss: -0.016344135627150536. Reward: 107\n",
            "Success in 533 episodes. Loss: -0.016344135627150536. Reward: 168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VE71NLiSHLf",
        "outputId": "91e66f68-1a73-4682-b570-9c6830d29017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 4. Loss: 0.56. Mean reward: 17.4. Reward: 36.\n",
            "Episode 8. Loss: 0.574. Mean reward: 15.78. Reward: 16.\n",
            "Episode 12. Loss: 0.57. Mean reward: 18.46. Reward: 32.\n",
            "Episode 16. Loss: 0.557. Mean reward: 18.35. Reward: 14.\n",
            "Episode 20. Loss: 0.564. Mean reward: 19.29. Reward: 15.\n",
            "Episode 24. Loss: 0.552. Mean reward: 19.72. Reward: 15.\n",
            "Episode 28. Loss: 0.582. Mean reward: 24.0. Reward: 84.\n",
            "Episode 32. Loss: 0.539. Mean reward: 24.67. Reward: 26.\n",
            "Episode 36. Loss: 0.523. Mean reward: 24.49. Reward: 36.\n",
            "Episode 40. Loss: 0.595. Mean reward: 23.71. Reward: 15.\n",
            "Episode 44. Loss: 0.555. Mean reward: 23.53. Reward: 13.\n",
            "Episode 48. Loss: 0.551. Mean reward: 25.31. Reward: 24.\n",
            "Episode 52. Loss: 0.529. Mean reward: 25.42. Reward: 20.\n",
            "Episode 56. Loss: 0.551. Mean reward: 26.51. Reward: 23.\n",
            "Episode 60. Loss: 0.523. Mean reward: 26.84. Reward: 17.\n",
            "Episode 64. Loss: 0.534. Mean reward: 27.86. Reward: 24.\n",
            "Episode 68. Loss: 0.521. Mean reward: 27.39. Reward: 31.\n",
            "Episode 72. Loss: 0.562. Mean reward: 27.64. Reward: 16.\n",
            "Episode 76. Loss: 0.538. Mean reward: 28.96. Reward: 46.\n",
            "Episode 80. Loss: 0.522. Mean reward: 29.21. Reward: 59.\n",
            "Episode 84. Loss: 0.53. Mean reward: 29.8. Reward: 22.\n",
            "Episode 88. Loss: 0.54. Mean reward: 30.44. Reward: 34.\n",
            "Episode 92. Loss: 0.556. Mean reward: 30.91. Reward: 31.\n",
            "Episode 96. Loss: 0.536. Mean reward: 31.27. Reward: 47.\n",
            "Episode 100. Loss: 0.518. Mean reward: 32.45. Reward: 33.\n",
            "Episode 104. Loss: 0.581. Mean reward: 32.84. Reward: 29.\n",
            "Episode 108. Loss: 0.531. Mean reward: 34.23. Reward: 37.\n",
            "Episode 112. Loss: 0.526. Mean reward: 34.65. Reward: 12.\n",
            "Episode 116. Loss: 0.557. Mean reward: 35.13. Reward: 26.\n",
            "Episode 120. Loss: 0.545. Mean reward: 36.19. Reward: 68.\n",
            "Episode 124. Loss: 0.555. Mean reward: 37.63. Reward: 135.\n",
            "Episode 128. Loss: 0.569. Mean reward: 37.96. Reward: 50.\n",
            "Episode 132. Loss: 0.571. Mean reward: 39.72. Reward: 45.\n",
            "Episode 136. Loss: 0.588. Mean reward: 40.63. Reward: 36.\n",
            "Episode 140. Loss: 0.589. Mean reward: 43.24. Reward: 26.\n",
            "Episode 144. Loss: 0.565. Mean reward: 45.0. Reward: 90.\n",
            "Episode 148. Loss: 0.56. Mean reward: 45.03. Reward: 32.\n",
            "Episode 152. Loss: 0.572. Mean reward: 46.57. Reward: 88.\n",
            "Episode 156. Loss: 0.617. Mean reward: 47.82. Reward: 107.\n",
            "Episode 160. Loss: 0.599. Mean reward: 49.38. Reward: 81.\n",
            "Episode 164. Loss: 0.555. Mean reward: 48.71. Reward: 25.\n",
            "Episode 168. Loss: 0.582. Mean reward: 50.71. Reward: 17.\n",
            "Episode 172. Loss: 0.62. Mean reward: 51.84. Reward: 35.\n",
            "Episode 176. Loss: 0.643. Mean reward: 50.7. Reward: 17.\n",
            "Episode 180. Loss: 0.656. Mean reward: 51.1. Reward: 33.\n",
            "Episode 184. Loss: 0.647. Mean reward: 52.49. Reward: 77.\n",
            "Episode 188. Loss: 0.621. Mean reward: 52.71. Reward: 80.\n",
            "Episode 192. Loss: 0.641. Mean reward: 52.62. Reward: 25.\n",
            "Episode 196. Loss: 0.64. Mean reward: 53.04. Reward: 54.\n",
            "Episode 200. Loss: 0.618. Mean reward: 53.61. Reward: 92.\n",
            "Episode 204. Loss: 0.658. Mean reward: 56.73. Reward: 104.\n",
            "Episode 208. Loss: 0.657. Mean reward: 59.72. Reward: 89.\n",
            "Episode 212. Loss: 0.629. Mean reward: 61.79. Reward: 69.\n",
            "Episode 216. Loss: 0.668. Mean reward: 63.38. Reward: 66.\n",
            "Episode 220. Loss: 0.625. Mean reward: 65.71. Reward: 91.\n",
            "Episode 224. Loss: 0.654. Mean reward: 66.33. Reward: 202.\n",
            "Episode 228. Loss: 0.664. Mean reward: 68.59. Reward: 136.\n",
            "Episode 232. Loss: 0.67. Mean reward: 70.18. Reward: 56.\n",
            "Episode 236. Loss: 0.676. Mean reward: 71.85. Reward: 124.\n",
            "Episode 240. Loss: 0.698. Mean reward: 73.39. Reward: 130.\n",
            "Episode 244. Loss: 0.649. Mean reward: 78.35. Reward: 73.\n",
            "Episode 248. Loss: 0.66. Mean reward: 83.31. Reward: 304.\n",
            "Episode 252. Loss: 0.662. Mean reward: 90.16. Reward: 36.\n",
            "Episode 256. Loss: 0.653. Mean reward: 93.07. Reward: 153.\n",
            "Episode 260. Loss: 0.67. Mean reward: 97.82. Reward: 133.\n",
            "Success in 263 episodes. Loss: 0.66982. Reward: 187.\n"
          ]
        }
      ],
      "source": [
        "#this is what you wrote as A2C instead\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_\n",
        "import numpy as np\n",
        "\n",
        "GAMMA = 0.99\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "\n",
        "class XavierLinear(nn.Linear):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        xavier_uniform_(self.weight)\n",
        "\n",
        "\n",
        "class VPG(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(VPG, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            XavierLinear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            XavierLinear(128, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.policy_head = XavierLinear(128, output_size)\n",
        "        self.value_head = XavierLinear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return F.softmax(self.policy_head(x), dim=-1), self.value_head(x)\n",
        "\n",
        "\n",
        "def run_episode(model, env):\n",
        "    obs = env.reset()[0]\n",
        "    obs = torch.Tensor(env.reset()[0]).to(DEVICE)\n",
        "    te = tr = False\n",
        "    rewards, values, outputs, actions = [], [], [], []\n",
        "    while not (te or tr):\n",
        "        probs, value = model(obs)\n",
        "        action = probs.multinomial(1).item()\n",
        "        obs, r, te, tr, _ = env.step(action)\n",
        "        obs = torch.Tensor(obs).to(DEVICE)\n",
        "        if (te or tr):\n",
        "            r = 0 # Here I also tried -1 if the pole falls and 0 otherwise\n",
        "        rewards.append(r)\n",
        "        values.append(value)\n",
        "        outputs.append(probs)\n",
        "        actions.append(action)\n",
        "    return torch.Tensor(rewards).to(DEVICE), torch.Tensor(values).to(DEVICE), torch.concatenate(outputs).reshape(len(rewards), 2), actions\n",
        "\n",
        "def discount_rewards(rewards):\n",
        "    discounted_r = torch.zeros_like(rewards)\n",
        "    additive_r = 0\n",
        "    for idx in range(len(rewards)-1, -1, -1):\n",
        "        to_add = GAMMA * additive_r\n",
        "        additive_r = to_add + rewards[idx]\n",
        "        discounted_r[idx] = additive_r\n",
        "    return discounted_r.to(DEVICE)\n",
        "\n",
        "def loss_function(discounted_r, values, probs, actions):\n",
        "    logprobs = torch.log(probs)\n",
        "    selected = logprobs[range(probs.shape[0]), actions]\n",
        "    discounted_r = (discounted_r - discounted_r.mean()) / discounted_r.std()\n",
        "    v = values.detach()\n",
        "    advantage = discounted_r - v\n",
        "    weighted = selected * advantage\n",
        "\n",
        "    value_loss = F.mse_loss(values, discounted_r.detach())\n",
        "\n",
        "    return -weighted.mean() + (value_loss.mean()*0.5)\n",
        "\n",
        "# The actual training loop:\n",
        "\n",
        "episode_total_reward = 0\n",
        "batch_losses = torch.Tensor().to(DEVICE)\n",
        "batch_actions = []\n",
        "batch_disc_r = torch.Tensor().to(DEVICE)\n",
        "batch_values = torch.Tensor().to(DEVICE)\n",
        "batch_probs = torch.Tensor().to(DEVICE)\n",
        "best_ep_reward = 0\n",
        "losses, ep_total_lenghts = [], [0]\n",
        "\n",
        "episodes = 0\n",
        "TARGET_REWARD = 100#400\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "model = VPG(env.observation_space.shape[0],\n",
        "            2).to(DEVICE)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "while np.array(ep_total_lenghts)[-100:].mean() < TARGET_REWARD:\n",
        "    rewards, values, probs, actions = run_episode(model, env)\n",
        "    discounted_r = discount_rewards(rewards)\n",
        "    episode_total_reward = rewards.shape[0]\n",
        "    ep_total_lenghts.append(episode_total_reward)\n",
        "    episodes += 1\n",
        "    batch_actions += actions\n",
        "    batch_disc_r = torch.concatenate([batch_disc_r, discounted_r])\n",
        "    batch_values = torch.concatenate([batch_values, values])\n",
        "    batch_probs = torch.concatenate([batch_probs, probs])\n",
        "\n",
        "    if episodes % BATCH_SIZE == 0:\n",
        "        loss = loss_function(batch_disc_r, batch_values, batch_probs, batch_actions)\n",
        "        losses.append(loss.item())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.001)\n",
        "        optim.step()\n",
        "        batch_actions = []\n",
        "        batch_disc_r = torch.Tensor().to(DEVICE)\n",
        "        batch_values = torch.Tensor().to(DEVICE)\n",
        "        batch_probs = torch.Tensor().to(DEVICE)\n",
        "        print(f\"Episode {episodes}. Loss: {np.round(loss.detach().item(), decimals=3)}. Mean reward: {np.round(np.array(ep_total_lenghts)[-100:].mean(), decimals=2)}. Reward: {episode_total_reward}.\")\n",
        "print(f\"Success in {episodes} episodes. Loss: {np.round(loss.detach().item(), decimals=5)}. Reward: {episode_total_reward}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "GAMMA = 0.99\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "\n",
        "class XavierLinear(nn.Linear):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        xavier_uniform_(self.weight)\n",
        "\n",
        "\n",
        "class VPG(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(VPG, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            XavierLinear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            XavierLinear(128, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softmax(self.net(x), dim=0)\n",
        "\n",
        "\n",
        "def run_episode(model, env):\n",
        "    obs = env.reset()[0]\n",
        "    obs = torch.Tensor(env.reset()[0]).to(DEVICE)\n",
        "    te = tr = False\n",
        "    rewards, outputs, actions = [], [], []\n",
        "    while not (te or tr):\n",
        "        probs = model(obs)\n",
        "        action = probs.multinomial(1).item()\n",
        "        obs, r, te, tr, _ = env.step(action)\n",
        "        obs = torch.Tensor(obs).to(DEVICE)\n",
        "        if (te or tr):\n",
        "            r = 0 # Here I also tried -1 if the pole falls and 0 otherwise\n",
        "        rewards.append(r)\n",
        "        outputs.append(probs)\n",
        "        actions.append(action)\n",
        "    return torch.Tensor(rewards).to(DEVICE), torch.concatenate(outputs).reshape(len(rewards), 2), actions\n",
        "\n",
        "def discount_rewards(rewards):\n",
        "    discounted_r = torch.zeros_like(rewards)\n",
        "    additive_r = 0\n",
        "    for idx in range(len(rewards)-1, -1, -1):\n",
        "        to_add = GAMMA * additive_r\n",
        "        additive_r = to_add + rewards[idx]\n",
        "        discounted_r[idx] = additive_r\n",
        "    return discounted_r.to(DEVICE)\n",
        "\n",
        "def loss_function(discounted_r, probs, actions):\n",
        "    selected = Categorical(probs).log_prob(torch.Tensor(actions).to(DEVICE))\n",
        "    discounted_r = (discounted_r - discounted_r.mean()) / discounted_r.std()\n",
        "    weighted = selected * discounted_r\n",
        "    return -weighted.sum()\n",
        "\n",
        "# The actual training loop:\n",
        "\n",
        "episode_total_reward = 0\n",
        "batch_losses = torch.Tensor().to(DEVICE)\n",
        "batch_actions = []\n",
        "batch_disc_r = torch.Tensor().to(DEVICE)\n",
        "batch_probs = torch.Tensor().to(DEVICE)\n",
        "best_ep_reward = 0\n",
        "losses, ep_total_lenghts = [], [0]\n",
        "\n",
        "episodes = 0\n",
        "TARGET_REWARD = 100\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "model = VPG(env.observation_space.shape[0],\n",
        "            2).to(DEVICE)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "while np.array(ep_total_lenghts)[-100:].mean() < TARGET_REWARD:\n",
        "    rewards, probs, actions = run_episode(model, env)\n",
        "    discounted_r = discount_rewards(rewards)\n",
        "    episode_total_reward = rewards.shape[0]\n",
        "    ep_total_lenghts.append(episode_total_reward)\n",
        "    episodes += 1\n",
        "    batch_actions += actions\n",
        "    batch_disc_r = torch.concatenate([batch_disc_r, discounted_r])\n",
        "    batch_probs = torch.concatenate([batch_probs, probs])\n",
        "\n",
        "    if episodes % BATCH_SIZE == 0:\n",
        "        loss = loss_function(batch_disc_r, batch_probs, batch_actions)\n",
        "        losses.append(loss.item())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        batch_actions = []\n",
        "        batch_disc_r = torch.Tensor().to(DEVICE)\n",
        "        batch_probs = torch.Tensor().to(DEVICE)\n",
        "        print(f\"Episode {episodes}. Loss: {loss}. Reward: {episode_total_reward}\")\n",
        "print(f\"Success in {episodes} episodes. Loss: {loss}. Reward: {episode_total_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYlBv62TSOfj",
        "outputId": "80ac6096-f6f1-4f48-a415-cad0963b8736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 4. Loss: 0.5146181583404541. Reward: 17\n",
            "Episode 8. Loss: 0.34052616357803345. Reward: 12\n",
            "Episode 12. Loss: -1.1095318794250488. Reward: 12\n",
            "Episode 16. Loss: -0.1488935649394989. Reward: 19\n",
            "Episode 20. Loss: -0.3533565104007721. Reward: 16\n",
            "Episode 24. Loss: -0.4083758592605591. Reward: 29\n",
            "Episode 28. Loss: -0.6365123987197876. Reward: 12\n",
            "Episode 32. Loss: -0.4132089614868164. Reward: 46\n",
            "Episode 36. Loss: -0.38385558128356934. Reward: 27\n",
            "Episode 40. Loss: -2.031681776046753. Reward: 23\n",
            "Episode 44. Loss: -0.150732159614563. Reward: 16\n",
            "Episode 48. Loss: -1.3684639930725098. Reward: 32\n",
            "Episode 52. Loss: -0.0780143141746521. Reward: 10\n",
            "Episode 56. Loss: -1.5128607749938965. Reward: 27\n",
            "Episode 60. Loss: -0.8497529029846191. Reward: 21\n",
            "Episode 64. Loss: -1.2733001708984375. Reward: 18\n",
            "Episode 68. Loss: -5.7394819259643555. Reward: 13\n",
            "Episode 72. Loss: -1.0529409646987915. Reward: 22\n",
            "Episode 76. Loss: 0.9459303617477417. Reward: 19\n",
            "Episode 80. Loss: -2.1868011951446533. Reward: 12\n",
            "Episode 84. Loss: -0.3441394567489624. Reward: 22\n",
            "Episode 88. Loss: 0.8199041485786438. Reward: 37\n",
            "Episode 92. Loss: -0.05048727989196777. Reward: 40\n",
            "Episode 96. Loss: -1.9398256540298462. Reward: 38\n",
            "Episode 100. Loss: 0.12800443172454834. Reward: 31\n",
            "Episode 104. Loss: -1.366638422012329. Reward: 19\n",
            "Episode 108. Loss: -1.6965596675872803. Reward: 27\n",
            "Episode 112. Loss: 0.7088524103164673. Reward: 32\n",
            "Episode 116. Loss: -2.1611526012420654. Reward: 26\n",
            "Episode 120. Loss: 0.2752673625946045. Reward: 39\n",
            "Episode 124. Loss: -2.226121425628662. Reward: 43\n",
            "Episode 128. Loss: -1.8193491697311401. Reward: 22\n",
            "Episode 132. Loss: -2.266632556915283. Reward: 33\n",
            "Episode 136. Loss: 0.49828922748565674. Reward: 14\n",
            "Episode 140. Loss: 0.5613044500350952. Reward: 15\n",
            "Episode 144. Loss: -2.9801995754241943. Reward: 40\n",
            "Episode 148. Loss: 0.1973578929901123. Reward: 34\n",
            "Episode 152. Loss: -1.2980775833129883. Reward: 31\n",
            "Episode 156. Loss: -1.3451206684112549. Reward: 38\n",
            "Episode 160. Loss: 0.7005884647369385. Reward: 26\n",
            "Episode 164. Loss: -1.0020062923431396. Reward: 41\n",
            "Episode 168. Loss: 0.7599628567695618. Reward: 20\n",
            "Episode 172. Loss: -6.5428466796875. Reward: 33\n",
            "Episode 176. Loss: -2.6769778728485107. Reward: 38\n",
            "Episode 180. Loss: -3.0552234649658203. Reward: 30\n",
            "Episode 184. Loss: -2.1991159915924072. Reward: 35\n",
            "Episode 188. Loss: -5.3722405433654785. Reward: 31\n",
            "Episode 192. Loss: -0.7803961038589478. Reward: 22\n",
            "Episode 196. Loss: -1.9976074695587158. Reward: 41\n",
            "Episode 200. Loss: -0.6994917392730713. Reward: 34\n",
            "Episode 204. Loss: 4.054227352142334. Reward: 63\n",
            "Episode 208. Loss: -2.048506259918213. Reward: 30\n",
            "Episode 212. Loss: -3.049898624420166. Reward: 18\n",
            "Episode 216. Loss: -1.0276768207550049. Reward: 34\n",
            "Episode 220. Loss: -2.124340534210205. Reward: 19\n",
            "Episode 224. Loss: 1.2018157243728638. Reward: 46\n",
            "Episode 228. Loss: 2.3885092735290527. Reward: 21\n",
            "Episode 232. Loss: -1.5137354135513306. Reward: 22\n",
            "Episode 236. Loss: -1.6291298866271973. Reward: 24\n",
            "Episode 240. Loss: 2.0122063159942627. Reward: 16\n",
            "Episode 244. Loss: -3.1999130249023438. Reward: 19\n",
            "Episode 248. Loss: -0.43157637119293213. Reward: 35\n",
            "Episode 252. Loss: -0.9100279808044434. Reward: 21\n",
            "Episode 256. Loss: -3.5465621948242188. Reward: 19\n",
            "Episode 260. Loss: -2.102839469909668. Reward: 50\n",
            "Episode 264. Loss: 2.1585493087768555. Reward: 26\n",
            "Episode 268. Loss: -0.6627815961837769. Reward: 51\n",
            "Episode 272. Loss: -2.0001773834228516. Reward: 72\n",
            "Episode 276. Loss: -5.296380043029785. Reward: 20\n",
            "Episode 280. Loss: -5.536765098571777. Reward: 20\n",
            "Episode 284. Loss: -3.9034128189086914. Reward: 35\n",
            "Episode 288. Loss: -4.853715896606445. Reward: 25\n",
            "Episode 292. Loss: -2.463289499282837. Reward: 26\n",
            "Episode 296. Loss: 0.1513962745666504. Reward: 30\n",
            "Episode 300. Loss: -0.9008457660675049. Reward: 52\n",
            "Episode 304. Loss: 0.5596764087677002. Reward: 34\n",
            "Episode 308. Loss: -2.182821273803711. Reward: 23\n",
            "Episode 312. Loss: -3.0803654193878174. Reward: 105\n",
            "Episode 316. Loss: 0.5078532695770264. Reward: 60\n",
            "Episode 320. Loss: -0.5776301622390747. Reward: 56\n",
            "Episode 324. Loss: 0.4204897880554199. Reward: 47\n",
            "Episode 328. Loss: -1.9226073026657104. Reward: 37\n",
            "Episode 332. Loss: -2.0664780139923096. Reward: 18\n",
            "Episode 336. Loss: -6.0125603675842285. Reward: 43\n",
            "Episode 340. Loss: -11.298139572143555. Reward: 85\n",
            "Episode 344. Loss: 0.5561904907226562. Reward: 43\n",
            "Episode 348. Loss: 2.0969467163085938. Reward: 65\n",
            "Episode 352. Loss: 1.0209763050079346. Reward: 35\n",
            "Episode 356. Loss: 0.5649971961975098. Reward: 28\n",
            "Episode 360. Loss: -2.078683853149414. Reward: 71\n",
            "Episode 364. Loss: -2.787393569946289. Reward: 35\n",
            "Episode 368. Loss: 1.4616400003433228. Reward: 50\n",
            "Episode 372. Loss: -2.9358394145965576. Reward: 17\n",
            "Episode 376. Loss: 0.7330116033554077. Reward: 26\n",
            "Episode 380. Loss: -1.170283555984497. Reward: 87\n",
            "Episode 384. Loss: -2.2284770011901855. Reward: 38\n",
            "Episode 388. Loss: 1.934763789176941. Reward: 53\n",
            "Episode 392. Loss: -3.6661667823791504. Reward: 41\n",
            "Episode 396. Loss: -2.2812671661376953. Reward: 139\n",
            "Episode 400. Loss: -2.7063727378845215. Reward: 47\n",
            "Episode 404. Loss: -0.9832134246826172. Reward: 64\n",
            "Episode 408. Loss: -2.6293458938598633. Reward: 43\n",
            "Episode 412. Loss: -1.9631496667861938. Reward: 13\n",
            "Episode 416. Loss: -3.762984037399292. Reward: 82\n",
            "Episode 420. Loss: -9.869964599609375. Reward: 88\n",
            "Episode 424. Loss: -2.96876859664917. Reward: 66\n",
            "Episode 428. Loss: 0.29532790184020996. Reward: 51\n",
            "Episode 432. Loss: -0.3697512149810791. Reward: 63\n",
            "Episode 436. Loss: 1.623885154724121. Reward: 61\n",
            "Episode 440. Loss: -3.813572645187378. Reward: 45\n",
            "Episode 444. Loss: -3.160245418548584. Reward: 21\n",
            "Episode 448. Loss: -0.4380338191986084. Reward: 57\n",
            "Episode 452. Loss: -2.343465566635132. Reward: 71\n",
            "Episode 456. Loss: 1.5381741523742676. Reward: 35\n",
            "Episode 460. Loss: 3.4208755493164062. Reward: 72\n",
            "Episode 464. Loss: -3.07839298248291. Reward: 103\n",
            "Episode 468. Loss: 4.373362064361572. Reward: 39\n",
            "Episode 472. Loss: -4.6863627433776855. Reward: 50\n",
            "Episode 476. Loss: -6.683560848236084. Reward: 58\n",
            "Episode 480. Loss: 1.554173231124878. Reward: 114\n",
            "Episode 484. Loss: -5.267746925354004. Reward: 76\n",
            "Episode 488. Loss: -5.675867557525635. Reward: 45\n",
            "Episode 492. Loss: -4.641866207122803. Reward: 64\n",
            "Episode 496. Loss: -10.45451831817627. Reward: 60\n",
            "Episode 500. Loss: 3.382713556289673. Reward: 116\n",
            "Episode 504. Loss: -1.7662549018859863. Reward: 41\n",
            "Episode 508. Loss: -11.547432899475098. Reward: 74\n",
            "Episode 512. Loss: -1.0807383060455322. Reward: 105\n",
            "Episode 516. Loss: -3.0060887336730957. Reward: 100\n",
            "Episode 520. Loss: -7.759438514709473. Reward: 19\n",
            "Episode 524. Loss: -9.22108268737793. Reward: 76\n",
            "Episode 528. Loss: -2.42887282371521. Reward: 29\n",
            "Episode 532. Loss: 0.37338054180145264. Reward: 68\n",
            "Episode 536. Loss: -1.8199989795684814. Reward: 63\n",
            "Episode 540. Loss: -8.928413391113281. Reward: 82\n",
            "Episode 544. Loss: -3.2057948112487793. Reward: 92\n",
            "Episode 548. Loss: -8.846211433410645. Reward: 96\n",
            "Episode 552. Loss: -7.459216594696045. Reward: 59\n",
            "Episode 556. Loss: -6.865462303161621. Reward: 202\n",
            "Episode 560. Loss: -7.2611002922058105. Reward: 45\n",
            "Episode 564. Loss: -2.9925849437713623. Reward: 110\n",
            "Episode 568. Loss: -8.714081764221191. Reward: 99\n",
            "Episode 572. Loss: -1.0868887901306152. Reward: 176\n",
            "Episode 576. Loss: 0.8524904251098633. Reward: 80\n",
            "Episode 580. Loss: -6.048588752746582. Reward: 50\n",
            "Episode 584. Loss: -2.6968889236450195. Reward: 50\n",
            "Episode 588. Loss: -11.199060440063477. Reward: 55\n",
            "Episode 592. Loss: -10.586915969848633. Reward: 112\n",
            "Episode 596. Loss: -1.0258030891418457. Reward: 120\n",
            "Episode 600. Loss: -6.872090816497803. Reward: 36\n",
            "Episode 604. Loss: -4.075563430786133. Reward: 44\n",
            "Episode 608. Loss: -6.9169111251831055. Reward: 78\n",
            "Episode 612. Loss: -13.873279571533203. Reward: 326\n",
            "Episode 616. Loss: -9.376381874084473. Reward: 160\n",
            "Success in 618 episodes. Loss: -9.376381874084473. Reward: 175\n"
          ]
        }
      ]
    }
  ]
}